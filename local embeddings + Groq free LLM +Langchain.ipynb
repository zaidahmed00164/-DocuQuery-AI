{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7440928c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1701b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 documents.\n",
      "Split into 71 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaid ahmed\\AppData\\Local\\Temp\\ipykernel_10628\\679499834.py:21: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model working. Vector length: 768\n",
      "Vector store created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Load documents\n",
    "loader = PyPDFDirectoryLoader(r\"C:\\Users\\zaid ahmed\\OneDrive\\Desktop\\data\")\n",
    "the_text = loader.load()\n",
    "print(f\"Loaded {len(the_text)} documents.\")\n",
    "\n",
    "# 2. Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(the_text)\n",
    "print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "# 3. Verify chunks are non-empty\n",
    "if not docs or not docs[0].page_content.strip():\n",
    "    raise ValueError(\"No valid text chunks found after splitting!\")\n",
    "\n",
    "# 4. Test embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "try:\n",
    "    test_embedding = embeddings.embed_query(\"Test\")\n",
    "    print(\"Embedding model working. Vector length:\", len(test_embedding))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Embedding model failed: {e}\")\n",
    "\n",
    "# 5. Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"ollama_embeds\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "print(\"Vector store created!\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831db79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from env import groq_api_key\n",
    "llm = ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name='llama-3.3-70b-versatile'\n",
    "    )\n",
    "\n",
    "\n",
    "#https://github.com/ollama/ollama mixtral-8x7b-32768\n",
    "\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb45281",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "789cde09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document appears to be a transcript of a conference call or earnings call\n",
      "for SJS Enterprises, a company that has recently made an acquisition of Walter\n",
      "Pack. The call discusses the company's financial performance, guidance, and\n",
      "strategy, including the impact of the acquisition on their earnings and margins.\n",
      "The participants also discuss the company's product lines, including consumer\n",
      "durables and exports, and answer questions from analysts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import textwrap\n",
    "import gradio as gr\n",
    "\n",
    "# Test the architecture with a simple hard coded question\n",
    "response = rag_chain.invoke(\"What is this document about\")\n",
    "print(textwrap.fill(response, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8355a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f3f5abe22535037465.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f3f5abe22535037465.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the questions dynamic using a chat interface. Let's use gradio for this.\n",
    "def process_question(user_question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Directly using the user's question as input for rag_chain.invoke\n",
    "    response = rag_chain.invoke(user_question)\n",
    "\n",
    "    # Measure the response time\n",
    "    end_time = time.time()\n",
    "    response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
    "\n",
    "    # Combine the response and the response time into a single string\n",
    "    full_response = f\"{response}\\n\\n{response_time}\"\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# Setup the Gradio interface\n",
    "iface = gr.Interface(fn=process_question,\n",
    "                     inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
    "                     outputs=gr.Textbox(),\n",
    "                     title=\"Personal Knowledge Chat App\",\n",
    "                     description=\"Ask any question about your document, and get an answer along with the response time.\")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b41bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
